R#1
1. The 14 diseases and their corresponding ratios in the CTRG dataset are as follows: lung lesion (57.2%), enlarged cardio mediastinum (35.0%), pleural other (24.6%), pleural effusion (12.3%), cardiomegaly (8.0%), support devices (6.2%), no finding (4.5%), consolidation (4.2%), lung opacity (1.6%), fracture (1.0%), atelectasis (0.7%), edema (0.6%), pneumonia (0.0%), and pneumothorax (0.0%). We used all 14 diseases for training and metrics computation. However, in the experiment shown in Figure 2, most methods achieved an F1 score of 0.0 for four diseases with particularly low ratios. Therefore, we excluded these diseases from Figure 2.
2. For the dataset split, we used uniform sampling to maintain similar disease ratios between the training and test sets. Afterward, we verified that the disease distribution remained consistent, even for diseases with very low abnormality rates.
3. The "no finding" category means CheXbert found no disease. The baseline F1 score is 0% because it failed to identify any true positive samples in this category. According to the F1 score equation, if there are no true positive samples, the F1 score is zero.
4. "We selected 30 CT scans at regular intervals" means we chose 30 slices at regular intervals within each CT volume as the input. I apologize for any confusion.
#TODO
5. Without DAA, the model is more cautious, leading to higher precision but lower recall as it misses less obvious abnormalities. Incorporating DAA allows the model to diagnose diseases independently, achieving a better balance between precision and recall.
6. The average score in Figure 2 represents the mean F1 score of the 8 diseases shown.
7. Thank you for your valuable suggestions. We will reorganize the results section and revise our diagrams for clarity in the final version.

R#3
#TODO
1. We use normal distribution random initialization for disease prototypes, providing flexibility in capturing disease information from diagnostic labels.
2. During testing, diagnostic prompts are based on the similarity between disease features and learned prototypes, not diagnostic labels. Achieving 100% accuracy in diagnostic prompts is challenging, so we combined visual features with diagnostic information as prompts for the LLM. This allows the LLM to use both types of information to generate more accurate reports. Although we use diagnostic labels during training, which may seem unfair compared to methods like R2Gen and R2GenCMN that don't, we still significantly outperform other methods that do use diagnostic labels, such as M2KT, PromptMRG, and SL-DG. This demonstrates the effectiveness of our approach.
3. Since abnormality information is more valuable in clinical settings, we treat all non-abnormal samples as normal to enhance the model's ability to recognize distinct abnormalities.
4. For CE evaluation, we focus on disease identification, as in previous works. Providing diagnostic prompts does not cause information leakage, as we do not use labels during testing; the prompts are generated based on disease features and learned prototypes.
5. The source code will be released upon acceptance.

R#4
1. Current work on 3D report generation is limited, so we included some 2D methods for comparison. We also incorporated notable 3D methods, such as SL-DG, the first known work on 3D report generation, and RadFM, a strong baseline trained on a 3D dataset. We trained the latest model, "CT2Rep," on the CTRG dataset, but it produced identical reports for all test samples, resulting in poor metrics: 19.52 BL-4 and 0.036 F1 score. In contrast, our method, the first to bootstrap an LLM for 3D report generation, outperforms both the compared 2D and 3D methods.
2. Recently, a large-scale CT report dataset containing 50,188 CT volumes has been released. 